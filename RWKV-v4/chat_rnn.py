########################################################################################################
# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM
########################################################################################################

import numpy as np
import math, os
import time
import types
import copy
import torch
from lightning_lite.utilities import data
from torch import optim, nn
from torch.nn import functional as F
from torch.utils.data import DataLoader

from datetime import datetime

now = datetime.now() # current date and time

date_time = now.strftime("%m-%d-%Y-%H-%M-%S")

from src.utils import GREBE_TOKENIZER, GrebeDataset

TEMPERATURE = 1.0
top_p = 0.7
top_p_newline = 0.9 # only used in TOKEN_MODE = char

torch.backends.cudnn.benchmark = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cuda.matmul.allow_tf32 = True
np.set_printoptions(precision=4, suppress=True, linewidth=200)

########################################################################################################
# Step 1: set model
# 
# Set TOKEN_MODE to 'char' or 'bpe' if the model is trained by 'train.py' from scratch.
#
# Set TOKEN_MODE to 'pile' if you want to test pre-trained pile models.
########################################################################################################

TOKEN_MODE = 'pile'  # char / bpe / pile

n_layer = 6
n_embd = 512
ctx_len = 1024

if TOKEN_MODE == 'char':
    MODEL_NAME = 'trained-500'  # your trained model
    WORD_NAME = 'vocab'  # the .json vocab (generated by train.py)
    # set UNKNOWN_CHAR to the rarest token in your vocab.json, and all unknown tokens in your prompt will be denoted by it
    UNKNOWN_CHAR = ' '  # here we just set it to ' ' for simplicity

elif TOKEN_MODE == 'bpe':
    MODEL_NAME = 'trained-500'  # your trained model
    WORD_NAME = ['model-vocab.json', 'model-merges.txt']  # [vocab, merge] for your BPE model
    UNKNOWN_CHAR = None

elif TOKEN_MODE == 'pile':
    WORD_NAME = ['20B_tokenizer.json', '20B_tokenizer.json']
    UNKNOWN_CHAR = None

    # ---> you can set MODEL_NAME to your fine-tuned model <---

    MODEL_NAME = 'RWKV-4-Pile-169M-20220807-8023'
    #MODEL_NAME = 'RWKV-4-Pile-1B5-20220903-8040'
    # MODEL_NAME = 'trained-11'
    #n_layer = 24
    n_layer = 12
    #n_embd = 2048
    n_embd = 768
    ctx_len = 1024

    # MODEL_NAME = 'RWKV-4-Pile-430M-20220808-8066'
    # n_layer = 24
    # n_embd = 1024
    # ctx_len = 1024

    # MODEL_NAME = 'RWKV-4-Pile-1B5-20220903-8040'
    # n_layer = 24
    # n_embd = 2048
    # ctx_len = 1024    

os.environ['RWKV_FLOAT_MODE'] = 'fp32'  # 'bf16' / 'fp16' / 'fp32' (note: only using fp32 at this moment)
os.environ['RWKV_RUN_DEVICE'] = 'cpu'  # 'cpu' (already very fast) or 'cuda'
model_type = 'RWKV'  # 'RWKV' or 'RWKV-ffnPre'

ACTUAL_DEVICE = 'cuda'

########################################################################################################
# Step 2: set prompt & sampling stuffs
########################################################################################################

# context = 'A'
# context = "\nIn the"
# context = '\nSugar:'

NUM_TRIALS = 999
LENGTH_PER_TRIAL = 333

TEMPERATURE = 1.0
top_p = 0.7
top_p_newline = 0.9  # only used in TOKEN_MODE = char

DEBUG_DEBUG = False  # True False --> show softmax output

########################################################################################################

print(f'Loading {MODEL_NAME}...')

print("Here0")

from src.model_run import RWKV_RNN
from src.model_run import GREBE_RNN

#print("Here1")

model = GREBE_RNN(MODEL_NAME, ACTUAL_DEVICE, model_type, n_layer, n_embd, ctx_len, True)

#print("Here2")

tokenizer = GREBE_TOKENIZER(WORD_NAME)

import numpy as np

prompt = "\nWhat do you think about pandas? "

prompt_tokenized = tokenizer.tokenizer.encode(prompt)

aa = {}
bb = {}
pp = {}
xx = {}

for i in range(len(prompt_tokenized)):
    if i == len(prompt_tokenized)-1:
        break
    x, xx, aa, bb, pp = model([prompt_tokenized[i]], xx, aa, bb, pp)

#train_loader = DataLoader(dataset, shuffle=False, batch_size=batch_size)

best_model = None
best_loss = np.inf

next_token = prompt_tokenized[-1]

print(prompt)

for i in range(100):

    y_pred, xx, aa, bb, pp = model([next_token], xx, aa, bb, pp)

    char = tokenizer.sample_logits_bef(y_pred, 0, temperature=TEMPERATURE,
                                   top_p_usual=top_p, top_p_newline=top_p_newline)
    char = char.item()
    bef = char

    print(tokenizer.tokenizer.decode(int(char)), end="")

    next_token = char
