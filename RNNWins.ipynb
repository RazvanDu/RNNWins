{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  164093\n",
      "Total Vocab:  65\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163993\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n",
    "X = X / float(n_vocab)\n",
    "y = torch.tensor(dataY)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([163993, 100, 1]) torch.Size([163993])\n"
     ]
    }
   ],
   "source": [
    "lookback = 1\n",
    "print(X.shape, y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=100, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(100, n_vocab)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.linear(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20500/20500 [01:46<00:00, 192.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 55975.3789\n",
      "Epoch 1: Cross-entropy: 53499.9961\n",
      "Epoch 2: Cross-entropy: 51445.9766\n",
      "Epoch 3: Cross-entropy: 49987.8477\n",
      "Epoch 4: Cross-entropy: 48593.4180\n",
      "Epoch 5: Cross-entropy: 47658.7539\n",
      "Epoch 6: Cross-entropy: 46498.1055\n",
      "Epoch 7: Cross-entropy: 45960.4023\n",
      "Epoch 8: Cross-entropy: 45077.9453\n",
      "Epoch 9: Cross-entropy: 44462.9609\n",
      "Epoch 10: Cross-entropy: 44053.7812\n",
      "Epoch 11: Cross-entropy: 43504.8633\n",
      "Epoch 12: Cross-entropy: 43323.5625\n",
      "Epoch 13: Cross-entropy: 42822.4805\n",
      "Epoch 14: Cross-entropy: 42415.9219\n",
      "Epoch 15: Cross-entropy: 42298.1172\n",
      "Epoch 16: Cross-entropy: 41780.5938\n",
      "Epoch 17: Cross-entropy: 41651.9453\n",
      "Epoch 18: Cross-entropy: 41419.4219\n",
      "Epoch 19: Cross-entropy: 41470.1211\n",
      "Epoch 20: Cross-entropy: 41305.1914\n",
      "Epoch 21: Cross-entropy: 40805.1250\n",
      "Epoch 22: Cross-entropy: 40698.2734\n",
      "Epoch 23: Cross-entropy: 40451.4531\n",
      "Epoch 24: Cross-entropy: 40431.8242\n",
      "Epoch 25: Cross-entropy: 40370.4219\n",
      "Epoch 26: Cross-entropy: 40551.8320\n",
      "Epoch 27: Cross-entropy: 39835.2109\n",
      "Epoch 28: Cross-entropy: 39844.9297\n",
      "Epoch 29: Cross-entropy: 39864.0352\n",
      "Epoch 30: Cross-entropy: 39614.3164\n",
      "Epoch 31: Cross-entropy: 39448.1055\n",
      "Epoch 32: Cross-entropy: 39188.4375\n",
      "Epoch 33: Cross-entropy: 40510.0547\n",
      "Epoch 34: Cross-entropy: 39873.3281\n",
      "Epoch 35: Cross-entropy: 40852.4180\n",
      "Epoch 36: Cross-entropy: 39751.0664\n",
      "Epoch 37: Cross-entropy: 41624.0039\n",
      "Epoch 38: Cross-entropy: 39571.9453\n",
      "Epoch 39: Cross-entropy: 39278.4414\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "def my_collate(batch):\n",
    "\n",
    "    # Preparing input sequences\n",
    "    x = [item[0] for item in batch]\n",
    "    x = torch.stack(x)\n",
    "    # Preparing target values\n",
    "    y = [item[1] for item in batch]\n",
    "    y = torch.stack(y)\n",
    "\n",
    "    return [x, y]\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = LSTMModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = tqdm(data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=8))\n",
    "\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "\n",
    "n_epochs = 40\n",
    "for epoch in range(n_epochs):\n",
    "    model.float()\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch.float().to(device))\n",
    "        loss = loss_fn(y_pred.to(device), y_batch.long().to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch.float().to(device))\n",
    "            loss += loss_fn(y_pred.to(device), y_batch.long().to(device))\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
    "\n",
    "torch.save([best_model, char_to_int], \"single-char2.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "prompt = raw_text[start:start+seq_length]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xd alice was at the beach and  dx \n",
      "Prompt: \"alice was at the beach and \"\n",
      "the white rabbit rea oo the white rabbit ceaone and the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the white rabbit ald the\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "best_model, char_to_int = torch.load(\"single-char.pth\")\n",
    "n_vocab = len(char_to_int)\n",
    "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "\n",
    "model = LSTMModel()\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "filename = \"wonderland.txt\"\n",
    "seq_length = 100\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "prompt = 'alice was at the beach and '\n",
    "print(\"xd \" + prompt + \" dx \")\n",
    "pattern = [char_to_int[c] for c in prompt]\n",
    "\n",
    "model.eval()\n",
    "print('Prompt: \"%s\"' % prompt)\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        # format input array of int into PyTorch tensor\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        # generate logits as output from the model\n",
    "        prediction = model(x)\n",
    "        # convert logits into one character\n",
    "        index = int(prediction.argmax())\n",
    "        result = int_to_char[index]\n",
    "        print(result, end=\"\")\n",
    "        # append the new character into the prompt for the next iteration\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "print()\n",
    "print(\"Done.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
